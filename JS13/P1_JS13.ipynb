{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzUoXZyl4PbXoDj5oVFTjg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Nama : Aditya Atadewa  \n","Kelas : TI 3G  \n","NIM : 2341720174  \n","Absen : 01  "],"metadata":{"id":"XvGAdV1l0mwO"}},{"cell_type":"markdown","source":["# Praktikum 1\n","\n","Praktikum 1 ini akan membuat JST sederhana (2 layer) dengan forward pass dan backpropagation manual."],"metadata":{"id":"BVZ07fViZvWr"}},{"cell_type":"markdown","source":["## Langkah:\n","\n","1. Buat dataset sederhana (XOR).\n","2. Inisialisasi bobot dan bias.\n","3. Implementasikan forward pass.\n","4. Hitung error dan lakukan backpropagation.\n","5. Update bobot menggunakan gradient descent."],"metadata":{"id":"5NGJ7nj2Z3vL"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Dataset XOR\n","X = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y = np.array([[0],[1],[1],[0]])\n","\n","# Parameter\n","input_size = 2\n","hidden_size = 2\n","output_size = 1\n","lr = 0.1\n","\n","# Inisialisasi bobot\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.zeros((1, hidden_size))\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.zeros((1, output_size))\n","\n","# Fungsi aktivasi\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","# Training\n","for epoch in range(10000):\n","    # Forward pass\n","    z1 = np.dot(X, W1) + b1\n","    a1 = sigmoid(z1)\n","    z2 = np.dot(a1, W2) + b2\n","    a2 = sigmoid(z2)\n","\n","    # Hitung error\n","    error = y - a2\n","\n","    # Backpropagation\n","    d_a2 = error * sigmoid_derivative(a2)\n","    d_W2 = np.dot(a1.T, d_a2)\n","    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n","\n","    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n","    d_W1 = np.dot(X.T, d_a1)\n","    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n","\n","    # Update bobot\n","    W1 += lr * d_W1\n","    b1 += lr * d_b1\n","    W2 += lr * d_W2\n","    b2 += lr * d_b2\n","\n","    if epoch % 1000 == 0:\n","        loss = np.mean(np.square(error))\n","        print(f\"Epoch {epoch}, Loss: {loss}\")\n","\n","# Output akhir\n","print(\"Prediksi:\")\n","print(a2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmS6MF9CZ9yM","executionInfo":{"status":"ok","timestamp":1764081979531,"user_tz":-420,"elapsed":566,"user":{"displayName":"Aditya Atadewa","userId":"07837304634321480627"}},"outputId":"06a24196-14df-418a-c72e-1da8dad13af0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.30373242946165585\n","Epoch 1000, Loss: 0.22417868872492613\n","Epoch 2000, Loss: 0.19008113732506882\n","Epoch 3000, Loss: 0.17754988915251804\n","Epoch 4000, Loss: 0.09518250996842813\n","Epoch 5000, Loss: 0.019593632566868493\n","Epoch 6000, Loss: 0.00916612508981996\n","Epoch 7000, Loss: 0.005793137985911435\n","Epoch 8000, Loss: 0.0041815136383882\n","Epoch 9000, Loss: 0.0032500277825114323\n","Prediksi:\n","[[0.0566466 ]\n"," [0.95350402]\n"," [0.94853601]\n"," [0.05072432]]\n"]}]},{"cell_type":"markdown","source":["# Tugas 1"],"metadata":{"id":"lgWUXNAvaCGT"}},{"cell_type":"markdown","source":["## 1. Ubah jumlah neuron hidden layer menjadi 3."],"metadata":{"id":"swdPXq7BaDb0"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Dataset XOR\n","X = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y = np.array([[0],[1],[1],[0]])\n","\n","# Parameter\n","input_size = 2\n","hidden_size = 3   # DIUBAH dari 2 â†’ 3\n","output_size = 1\n","lr = 0.1\n","\n","# Inisialisasi bobot\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.zeros((1, hidden_size))\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.zeros((1, output_size))\n","\n","# Aktivasi sigmoid\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","# Training\n","for epoch in range(10000):\n","    # Forward pass\n","    z1 = np.dot(X, W1) + b1\n","    a1 = sigmoid(z1)\n","    z2 = np.dot(a1, W2) + b2\n","    a2 = sigmoid(z2)\n","\n","    # Error\n","    error = y - a2\n","\n","    # Backprop\n","    d_a2 = error * sigmoid_derivative(a2)\n","    d_W2 = np.dot(a1.T, d_a2)\n","    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n","\n","    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n","    d_W1 = np.dot(X.T, d_a1)\n","    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n","\n","    # Update bobot\n","    W1 += lr * d_W1\n","    b1 += lr * d_b1\n","    W2 += lr * d_W2\n","    b2 += lr * d_b2\n","\n","    if epoch % 1000 == 0:\n","        loss = np.mean(np.square(error))\n","        print(f\"Epoch {epoch}, Loss: {loss}\")\n","\n","print(\"\\nPrediksi (hidden=3, sigmoid):\")\n","print(a2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WV_TbUdmaHoL","executionInfo":{"status":"ok","timestamp":1764081980087,"user_tz":-420,"elapsed":552,"user":{"displayName":"Aditya Atadewa","userId":"07837304634321480627"}},"outputId":"de9e86c7-0735-4e99-8d10-d3e7fe1dc22b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.32684267998508215\n","Epoch 1000, Loss: 0.2440004401106532\n","Epoch 2000, Loss: 0.20254307742066419\n","Epoch 3000, Loss: 0.1543025881682945\n","Epoch 4000, Loss: 0.11549193034468697\n","Epoch 5000, Loss: 0.03181165111007862\n","Epoch 6000, Loss: 0.014446834760695262\n","Epoch 7000, Loss: 0.008633229637021116\n","Epoch 8000, Loss: 0.005931372327187066\n","Epoch 9000, Loss: 0.004431561997727911\n","\n","Prediksi (hidden=3, sigmoid):\n","[[0.03639761]\n"," [0.93813867]\n"," [0.93934954]\n"," [0.07188041]]\n"]}]},{"cell_type":"markdown","source":["## 2. Bandingkan hasil loss dengan konfigurasi awal."],"metadata":{"id":"JaeYuHapaJQy"}},{"cell_type":"markdown","source":["### **Perbandingan Model**\n","\n","Berikut perbandingan antara model awal (hidden=2) dan model modifikasi pada Tugas 1 Soal 1 (hidden=3), keduanya menggunakan aktivasi sigmoid:\n","\n","\n","| Model               | Hidden Neuron | Aktivasi | Loss Akhir (epoch 9000) |\n","|---------------------|---------------|----------|--------------------------|\n","| Praktikum Awal      | 2             | Sigmoid  | **0.00325**              |\n","| Tugas 1 Nomor 1     | 3             | Sigmoid  | **0.00443**              |"],"metadata":{"id":"dtJsGHv0fGAe"}},{"cell_type":"markdown","source":["### **Analisis**\n","- Kedua model berhasil mempelajari fungsi XOR dengan baik, ditunjukkan oleh output prediksi yang sangat mendekati target.\n","- Secara teori, menambah neuron hidden layer meningkatkan kapasitas model. Namun pada dataset kecil seperti XOR, perbedaan performa tidak selalu signifikan.\n","- Pada percobaan ini, **loss akhir model hidden=2 justru sedikit lebih kecil** dibanding hidden=3.  \n","  Hal ini dapat dipengaruhi oleh:\n","  - inisialisasi bobot acak\n","  - dataset yang sangat kecil\n","  - karakteristik sigmoid yang cenderung mengalami saturasi."],"metadata":{"id":"SOfl-ooRfwXh"}},{"cell_type":"markdown","source":["\n","### **Kesimpulan**\n","\n","Baik model dengan hidden=2 maupun hidden=3 mampu mempelajari pola XOR dengan baik. Meskipun hidden=3 memiliki kapasitas lebih tinggi, loss akhirnya tidak selalu lebih kecil. Pada percobaan ini, model hidden=2 justru memberikan loss akhir sedikit lebih baik."],"metadata":{"id":"BExHa4XBfzbw"}},{"cell_type":"markdown","source":["## 3. Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."],"metadata":{"id":"3JCv5vnfcL5m"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Dataset XOR\n","X = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y = np.array([[0],[1],[1],[0]])\n","\n","# Parameter\n","input_size = 2\n","hidden_size = 3\n","output_size = 1\n","lr = 0.1\n","\n","# Inisialisasi bobot\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.zeros((1, hidden_size))\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.zeros((1, output_size))\n","\n","# Aktivasi\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","def relu_derivative(x):\n","    return np.where(x > 0, 1, 0)\n","\n","# Training\n","for epoch in range(10000):\n","    # Forward pass\n","    z1 = np.dot(X, W1) + b1\n","    a1 = relu(z1)          # HIDDEN: ReLU\n","    z2 = np.dot(a1, W2) + b2\n","    a2 = sigmoid(z2)       # OUTPUT: Sigmoid\n","\n","    # Error\n","    error = y - a2\n","\n","    # Backprop\n","    d_a2 = error * sigmoid_derivative(a2)\n","    d_W2 = np.dot(a1.T, d_a2)\n","    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n","\n","    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(a1)\n","    d_W1 = np.dot(X.T, d_a1)\n","    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n","\n","    # Update bobot\n","    W1 += lr * d_W1\n","    b1 += lr * d_b1\n","    W2 += lr * d_W2\n","    b2 += lr * d_b2\n","\n","    if epoch % 1000 == 0:\n","        loss = np.mean(np.square(error))\n","        print(f\"Epoch {epoch}, Loss: {loss}\")\n","\n","print(\"\\nPrediksi (hidden=3, ReLU):\")\n","print(a2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DMguymPGcNT_","executionInfo":{"status":"ok","timestamp":1764081980949,"user_tz":-420,"elapsed":842,"user":{"displayName":"Aditya Atadewa","userId":"07837304634321480627"}},"outputId":"3f125539-981a-43b0-b043-7ba1e2b398c1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.26401624269715745\n","Epoch 1000, Loss: 0.003787904227308319\n","Epoch 2000, Loss: 0.0015662906296563073\n","Epoch 3000, Loss: 0.0009575079381685195\n","Epoch 4000, Loss: 0.0006816573726979983\n","Epoch 5000, Loss: 0.0005261267568128151\n","Epoch 6000, Loss: 0.000426752007537184\n","Epoch 7000, Loss: 0.0003580529160303689\n","Epoch 8000, Loss: 0.00030791082451776786\n","Epoch 9000, Loss: 0.00026984414334509555\n","\n","Prediksi (hidden=3, ReLU):\n","[[0.0195649 ]\n"," [0.99017285]\n"," [0.99014621]\n"," [0.01956997]]\n"]}]},{"cell_type":"markdown","source":["### **Perbandingan Model**\n","Berikut perbandingan antara dua model dengan hidden=3 neuron, namun aktivasi berbeda:\n","\n","| Model                     | Hidden Neuron | Aktivasi Hidden | Loss Akhir (epoch 9000) |\n","|---------------------------|---------------|------------------|--------------------------|\n","| Tugas 1 Nomor 1 (sigmoid) | 3             | Sigmoid          | **0.00443**              |\n","| Tugas 1 Nomor 3 (ReLU)    | 3             | ReLU             | **0.00027**              |"],"metadata":{"id":"DwBgy8DkhSkT"}},{"cell_type":"markdown","source":["\n","### **Analisis**\n","- Menggunakan ReLU pada hidden layer menghasilkan **penurunan loss yang jauh lebih cepat dan lebih kecil** dibanding sigmoid.\n","- Pada epoch 1000 saja, model ReLU sudah mencapai loss sekitar **0.00378**, lebih kecil dari loss akhir sigmoid pada epoch 9000.\n","- Output prediksi model ReLU juga sangat mendekati target `[0, 1, 1, 0]`, menunjukkan model belajar lebih efektif.\n","- Secara intuitif:\n","  - **Sigmoid** memiliki masalah *vanishing gradient* ketika mendekati 0 atau 1.\n","  - **ReLU** memiliki gradien konstan pada nilai positif, sehingga gradien mengalir lebih kuat selama backpropagation."],"metadata":{"id":"KiNqdQnthZtz"}},{"cell_type":"markdown","source":["\n","### **Kesimpulan**\n","\n","Dengan konfigurasi hidden layer yang sama (3 neuron), penggunaan ReLU pada hidden layer memberikan performa yang jauh lebih baik dibanding sigmoid. Loss akhir turun signifikan dari 0.00443 menjadi 0.00027, menunjukkan ReLU lebih efisien dalam mengatasi masalah vanishing gradient dan mempercepat proses training"],"metadata":{"id":"VIZnrl9NhbsU"}}]}